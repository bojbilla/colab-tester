{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "from Dice21Env import Dice21Env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define a helper function to create Dense layers configured with the right\n",
    "# activation and kernel initializer.\n",
    "def dense_layer(num_units):\n",
    "    return tf.keras.layers.Dense(\n",
    "        num_units,\n",
    "        activation=tf.keras.activations.relu,\n",
    "        kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
    "            scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
    "\n",
    "\n",
    "def collect_step(environment, policy, buffer):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "    # Add trajectory to the replay buffer\n",
    "    buffer.add_batch(traj)\n",
    "\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "    for _ in range(steps):\n",
    "        collect_step(env, policy, buffer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.version.VERSION)\n",
    "\n",
    "num_iterations = 20000  # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 500  # @param {type:\"integer\"}\n",
    "collect_steps_per_iteration =  1 # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 32  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-4  # @param {type:\"number\"}\n",
    "log_interval = 500  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 50  # @param {type:\"integer\"}\n",
    "eval_interval = 200  # @param {type:\"integer\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_env = tf_py_environment.TFPyEnvironment(Dice21Env())\n",
    "eval_env = tf_py_environment.TFPyEnvironment(Dice21Env())\n",
    "\n",
    "fc_layer_params = (4, 4)\n",
    "action_tensor_spec = tensor_spec.from_spec(train_env.action_spec())\n",
    "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
    "\n",
    "# QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
    "# with `num_actions` units to generate one q_value per available action as\n",
    "# it's output.\n",
    "\n",
    "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
    "q_values_layer = tf.keras.layers.Dense(\n",
    "    num_actions,\n",
    "    activation=None,\n",
    "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "        minval=-0.03, maxval=0.03))\n",
    "q_net = sequential.Sequential(dense_layers + [q_values_layer])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n",
    "\n",
    "collect_data(train_env, random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                        train_env.action_spec()), replay_buffer, initial_collect_steps)\n",
    "\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3,\n",
    "    sample_batch_size=train_env.batch_size,\n",
    "    num_steps=2\n",
    ").prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%% TRAIN\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 200: Average Return = 0.0\n",
      "step = 400: Average Return = 0.0\n",
      "step = 600: Average Return = 0.0\n",
      "step = 800: Average Return = 0.0\n",
      "step = 1000: Average Return = 0.0\n",
      "step = 1200: Average Return = 0.0\n",
      "step = 1400: Average Return = 0.0\n",
      "step = 1600: Average Return = 0.0\n",
      "step = 1800: Average Return = 0.0\n",
      "step = 2000: Average Return = 0.0\n",
      "step = 2200: Average Return = 0.0\n",
      "step = 2400: Average Return = 0.0\n",
      "step = 2600: Average Return = 0.0\n",
      "step = 2800: Average Return = 0.0\n",
      "step = 3000: Average Return = 0.0\n",
      "step = 3200: Average Return = 0.0\n",
      "step = 3400: Average Return = 0.16100002825260162\n",
      "step = 3600: Average Return = 0.1963580846786499\n",
      "step = 3800: Average Return = 0.187866672873497\n",
      "step = 4000: Average Return = 0.22802671790122986\n",
      "step = 4200: Average Return = 0.23090381920337677\n",
      "step = 4400: Average Return = 0.3190229535102844\n",
      "step = 4600: Average Return = 0.3546009361743927\n",
      "step = 4800: Average Return = 0.46379244327545166\n",
      "step = 5000: Average Return = 0.7339914441108704\n",
      "step = 5200: Average Return = 0.8071848154067993\n",
      "step = 5400: Average Return = 0.656928539276123\n",
      "step = 5600: Average Return = 0.685140073299408\n",
      "step = 5800: Average Return = -0.11465997993946075\n",
      "step = 6000: Average Return = 0.7799981832504272\n",
      "step = 6200: Average Return = 0.8361884951591492\n",
      "step = 6400: Average Return = 0.15018190443515778\n",
      "step = 6600: Average Return = 0.7669848799705505\n",
      "step = 6800: Average Return = 0.7189734578132629\n",
      "step = 7000: Average Return = -0.0062285722233355045\n",
      "step = 7200: Average Return = -0.3545999526977539\n",
      "step = 7400: Average Return = 0.8458924889564514\n",
      "step = 7600: Average Return = 0.8357362151145935\n",
      "step = 7800: Average Return = 0.21411141753196716\n",
      "step = 8000: Average Return = 0.11210668832063675\n",
      "step = 8200: Average Return = 0.7832551598548889\n",
      "step = 8400: Average Return = 0.8678772449493408\n",
      "step = 8600: Average Return = 0.44440758228302\n",
      "step = 8800: Average Return = 0.685219943523407\n",
      "step = 9000: Average Return = 0.3727990388870239\n",
      "step = 9200: Average Return = 0.37016189098358154\n",
      "step = 9400: Average Return = 0.7261523604393005\n",
      "step = 9600: Average Return = 0.32900941371917725\n",
      "step = 9800: Average Return = 0.525399923324585\n",
      "step = 10000: Average Return = 0.36746475100517273\n",
      "step = 10200: Average Return = 0.834564208984375\n",
      "step = 10400: Average Return = 0.7709742784500122\n",
      "step = 10600: Average Return = 0.6530390381813049\n",
      "step = 10800: Average Return = 0.7786333560943604\n",
      "step = 11000: Average Return = 0.5737713575363159\n",
      "step = 11200: Average Return = 0.5651599764823914\n",
      "step = 11400: Average Return = 0.6843475103378296\n",
      "step = 11600: Average Return = 0.29119426012039185\n",
      "step = 11800: Average Return = 0.7254400849342346\n",
      "step = 12000: Average Return = 0.500357985496521\n",
      "step = 12200: Average Return = 0.5262923240661621\n",
      "step = 12400: Average Return = 0.48833712935447693\n",
      "step = 12600: Average Return = 0.723295271396637\n",
      "step = 12800: Average Return = 0.441850483417511\n",
      "step = 13000: Average Return = 0.7065525054931641\n",
      "step = 13200: Average Return = 0.6853398680686951\n",
      "step = 13400: Average Return = 0.8626629710197449\n",
      "step = 13600: Average Return = 0.8381211757659912\n",
      "step = 13800: Average Return = 0.8374209403991699\n",
      "step = 14000: Average Return = 0.8566485047340393\n",
      "step = 14200: Average Return = 0.8012506365776062\n",
      "step = 14400: Average Return = 0.7891496419906616\n",
      "step = 14600: Average Return = 0.08853717148303986\n",
      "step = 14800: Average Return = 0.4439951777458191\n",
      "step = 15000: Average Return = 0.6193856596946716\n",
      "step = 15200: Average Return = 0.46420764923095703\n",
      "step = 15400: Average Return = 0.7184534668922424\n",
      "step = 15600: Average Return = 0.17293907701969147\n",
      "step = 15800: Average Return = 0.5701019167900085\n",
      "step = 16000: Average Return = 0.5704018473625183\n",
      "step = 16200: Average Return = 0.7997658252716064\n",
      "step = 16400: Average Return = 0.5227627754211426\n",
      "step = 16600: Average Return = 0.21232666075229645\n",
      "step = 16800: Average Return = 0.8089497089385986\n",
      "step = 17000: Average Return = 0.6920667290687561\n",
      "step = 17200: Average Return = 0.5348237752914429\n",
      "step = 17400: Average Return = 0.8023830652236938\n",
      "step = 17600: Average Return = 0.6935514211654663\n",
      "step = 17800: Average Return = 0.7330991625785828\n",
      "step = 18000: Average Return = 0.8078773021697998\n",
      "step = 18200: Average Return = 0.44643235206604004\n",
      "step = 18400: Average Return = 0.820618212223053\n",
      "step = 18600: Average Return = 0.8326991200447083\n",
      "step = 18800: Average Return = 0.7736314535140991\n",
      "step = 19000: Average Return = 0.7356762886047363\n",
      "step = 19200: Average Return = 0.6484371423721313\n",
      "step = 19400: Average Return = 0.5774009227752686\n",
      "step = 19600: Average Return = 0.2122466266155243\n",
      "step = 19800: Average Return = 0.21601620316505432\n",
      "step = 20000: Average Return = 0.3652400076389313\n"
     ]
    }
   ],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10, debug=False):\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "        step_count = 0\n",
    "        while not time_step.is_last():\n",
    "            if debug:\n",
    "                print(f\"observation: {time_step.observation}\")\n",
    "            step_count += 1\n",
    "            action_step = policy.action(time_step)\n",
    "            if debug:\n",
    "                print(f\"action: {action_step}\")\n",
    "            time_step = environment.step(action_step.action)\n",
    "            if debug:\n",
    "                print(f\"reward: {time_step.reward}\")\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    _avg_return = total_return / num_episodes\n",
    "    return _avg_return.numpy()[0]\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for x in range(num_iterations):\n",
    "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "    collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n",
    "\n",
    "    # Sample a batch of data from the buffer and update the agent's network.\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = agent.train(experience).loss\n",
    "\n",
    "    step = agent.train_step_counter.numpy()\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return*21))\n",
    "        returns.append(avg_return)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%% Viz\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[3 3]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [3.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[2 2]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [2.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[6 6]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [6.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[4 4]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [4.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[3 3]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [3.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[1 1]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [1.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[2 2]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [2.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[2 2]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [2.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[3 3]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [3.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[1 1]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [1.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[3 3]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [3.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[5 5]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [5.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[6 6]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [6.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[1 1]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [1.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[2 2]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [2.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[3 3]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [3.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[1 1]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [1.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[4 4]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [4.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[1 1]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [1.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[2 2]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [2.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[4 4]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [4.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[6 6]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [6.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[5 5]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [5.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[3 3]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [3.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[2 2]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [2.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[6 6]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [6.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[5 5]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [5.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[1 1]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [1.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[2 2]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [2.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[3 3]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [3.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[1 1]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [1.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[6 6]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [6.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[5 5]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [5.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[6 6]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [6.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[5 5]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [5.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[4 4]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [4.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[5 5]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [5.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[2 2]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [2.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[3 3]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [3.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[4 4]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [4.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[1 1]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [1.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[3 3]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [3.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[5 5]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [5.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[5 5]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [5.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[1 1]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [1.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[3 3]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [3.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[4 4]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [4.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[6 6]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [6.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[1 1]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [1.]\n",
      "observation: [[0 0]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
      "reward: [0.001]\n",
      "observation: [[1 1]]\n",
      "action: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
      "reward: [1.]\n"
     ]
    }
   ],
   "source": [
    "res = compute_avg_return(eval_env, agent.policy, num_eval_episodes, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% Viz\n"
    }
   },
   "outputs": [],
   "source": [
    "iterations = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylim(top=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}