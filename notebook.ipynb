{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "from Dice21Env import Dice21Env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define a helper function to create Dense layers configured with the right\n",
    "# activation and kernel initializer.\n",
    "def dense_layer(num_units):\n",
    "    return tf.keras.layers.Dense(\n",
    "        num_units,\n",
    "        activation=tf.keras.activations.relu,\n",
    "        kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
    "            scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
    "\n",
    "\n",
    "def collect_step(environment, policy, buffer):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "    # Add trajectory to the replay buffer\n",
    "    buffer.add_batch(traj)\n",
    "\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "    for _ in range(steps):\n",
    "        collect_step(env, policy, buffer)\n",
    "\n",
    "\n",
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    _avg_return = total_return / num_episodes\n",
    "    return _avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.version.VERSION)\n",
    "\n",
    "num_iterations = 5000  # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 100  # @param {type:\"integer\"}\n",
    "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 16  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 50  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 50  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_env = tf_py_environment.TFPyEnvironment(Dice21Env())\n",
    "eval_env = tf_py_environment.TFPyEnvironment(Dice21Env())\n",
    "\n",
    "fc_layer_params = (30, 10)\n",
    "action_tensor_spec = tensor_spec.from_spec(train_env.action_spec())\n",
    "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
    "\n",
    "# QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
    "# with `num_actions` units to generate one q_value per available action as\n",
    "# it's output.\n",
    "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
    "q_values_layer = tf.keras.layers.Dense(\n",
    "    num_actions,\n",
    "    activation=None,\n",
    "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "        minval=-0.03, maxval=0.03),\n",
    "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
    "q_net = sequential.Sequential(dense_layers + [q_values_layer])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n",
    "\n",
    "collect_data(train_env, random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                        train_env.action_spec()), replay_buffer, initial_collect_steps)\n",
    "\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3,\n",
    "    sample_batch_size=batch_size,\n",
    "    num_steps=2\n",
    ").prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%% TRAIN\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-14-e9f50e03beb7>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;31m# Evaluate the agent's policy once before training.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 8\u001B[0;31m \u001B[0mavg_return\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcompute_avg_return\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0meval_env\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0magent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpolicy\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_eval_episodes\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      9\u001B[0m \u001B[0mreturns\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mavg_return\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-7-3ff516c634db>\u001B[0m in \u001B[0;36mcompute_avg_return\u001B[0;34m(environment, policy, num_episodes)\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     33\u001B[0m         \u001B[0;32mwhile\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mtime_step\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_last\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 34\u001B[0;31m             \u001B[0maction_step\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpolicy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtime_step\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     35\u001B[0m             \u001B[0mtime_step\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0menvironment\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction_step\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     36\u001B[0m             \u001B[0mepisode_return\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mtime_step\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreward\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/IdeaProjects/colab-tester/venv/lib/python3.8/site-packages/tf_agents/policies/tf_policy.py\u001B[0m in \u001B[0;36maction\u001B[0;34m(self, time_step, policy_state, seed)\u001B[0m\n\u001B[1;32m    322\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_automatic_state_reset\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    323\u001B[0m       \u001B[0mpolicy_state\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_maybe_reset_state\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtime_step\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpolicy_state\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 324\u001B[0;31m     \u001B[0mstep\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0maction_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtime_step\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtime_step\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpolicy_state\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpolicy_state\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mseed\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mseed\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    325\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    326\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mclip_action\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maction_spec\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/IdeaProjects/colab-tester/venv/lib/python3.8/site-packages/tf_agents/utils/common.py\u001B[0m in \u001B[0;36mwith_check_resource_vars\u001B[0;34m(*fn_args, **fn_kwargs)\u001B[0m\n\u001B[1;32m    183\u001B[0m         \u001B[0;31m# We're either in eager mode or in tf.function mode (no in-between); so\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    184\u001B[0m         \u001B[0;31m# autodep-like behavior is already expected of fn.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 185\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mfn_args\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mfn_kwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    186\u001B[0m       \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mresource_variables_enabled\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    187\u001B[0m         \u001B[0;32mraise\u001B[0m \u001B[0mRuntimeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mMISSING_RESOURCE_VARIABLES_ERROR\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/IdeaProjects/colab-tester/venv/lib/python3.8/site-packages/tf_agents/policies/tf_policy.py\u001B[0m in \u001B[0;36m_action\u001B[0;34m(self, time_step, policy_state, seed)\u001B[0m\n\u001B[1;32m    558\u001B[0m     \"\"\"\n\u001B[1;32m    559\u001B[0m     \u001B[0mseed_stream\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtfp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mutil\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSeedStream\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mseed\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mseed\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msalt\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'tf_agents_tf_policy'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 560\u001B[0;31m     \u001B[0mdistribution_step\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_distribution\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtime_step\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpolicy_state\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# pytype: disable=wrong-arg-types\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    561\u001B[0m     actions = tf.nest.map_structure(\n\u001B[1;32m    562\u001B[0m         \u001B[0;32mlambda\u001B[0m \u001B[0md\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mreparameterized_sampling\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msample\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0md\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mseed\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mseed_stream\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/IdeaProjects/colab-tester/venv/lib/python3.8/site-packages/tf_agents/policies/greedy_policy.py\u001B[0m in \u001B[0;36m_distribution\u001B[0;34m(self, time_step, policy_state)\u001B[0m\n\u001B[1;32m     78\u001B[0m       \u001B[0;32mreturn\u001B[0m \u001B[0mDeterministicWithLogProb\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mloc\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mgreedy_action\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     79\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 80\u001B[0;31m     distribution_step = self._wrapped_policy.distribution(\n\u001B[0m\u001B[1;32m     81\u001B[0m         time_step, policy_state)\n\u001B[1;32m     82\u001B[0m     return policy_step.PolicyStep(\n",
      "\u001B[0;32m~/IdeaProjects/colab-tester/venv/lib/python3.8/site-packages/tf_agents/policies/tf_policy.py\u001B[0m in \u001B[0;36mdistribution\u001B[0;34m(self, time_step, policy_state)\u001B[0m\n\u001B[1;32m    401\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_automatic_state_reset\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    402\u001B[0m       \u001B[0mpolicy_state\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_maybe_reset_state\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtime_step\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpolicy_state\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 403\u001B[0;31m     \u001B[0mstep\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_distribution\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtime_step\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtime_step\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpolicy_state\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpolicy_state\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    404\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0memit_log_probability\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    405\u001B[0m       \u001B[0;31m# This here is set only for compatibility with info_spec in constructor.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/IdeaProjects/colab-tester/venv/lib/python3.8/site-packages/tf_agents/policies/q_policy.py\u001B[0m in \u001B[0;36m_distribution\u001B[0;34m(self, time_step, policy_state)\u001B[0m\n\u001B[1;32m    151\u001B[0m           network_observation)\n\u001B[1;32m    152\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 153\u001B[0;31m     q_values, policy_state = self._q_network(\n\u001B[0m\u001B[1;32m    154\u001B[0m         \u001B[0mnetwork_observation\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnetwork_state\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpolicy_state\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    155\u001B[0m         step_type=time_step.step_type)\n",
      "\u001B[0;32m~/IdeaProjects/colab-tester/venv/lib/python3.8/site-packages/tf_agents/networks/network.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, inputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    390\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    391\u001B[0m     \u001B[0;31m# Convert *args, **kwargs to a canonical kwarg representation.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 392\u001B[0;31m     normalized_kwargs = tf_inspect.getcallargs(\n\u001B[0m\u001B[1;32m    393\u001B[0m         self.call, inputs, *args, **kwargs)\n\u001B[1;32m    394\u001B[0m     \u001B[0;31m# TODO(b/156315434): Rename network_state to just state.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/IdeaProjects/colab-tester/venv/lib/python3.8/site-packages/tensorflow/python/util/tf_inspect.py\u001B[0m in \u001B[0;36mgetcallargs\u001B[0;34m(*func_and_positional, **named)\u001B[0m\n\u001B[1;32m    258\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    259\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 260\u001B[0;31m \u001B[0;32mdef\u001B[0m \u001B[0mgetcallargs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mfunc_and_positional\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mnamed\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    261\u001B[0m   \"\"\"TFDecorator-aware replacement for inspect.getcallargs.\n\u001B[1;32m    262\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for x in range(num_iterations):\n",
    "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "    collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n",
    "\n",
    "    # Sample a batch of data from the buffer and update the agent's network.\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = agent.train(experience).loss\n",
    "\n",
    "    step = agent.train_step_counter.numpy()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "        returns.append(avg_return)\n",
    "        \n",
    "print(\"Stap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% Viz\n"
    }
   },
   "outputs": [],
   "source": [
    "iterations = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylim(top=250)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}